{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cfe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfoNetEstimator.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "import torch\n",
    "\n",
    "\n",
    "class InfoNetEstimator:\n",
    "    def __init__(self, model, proj_num=1024, seq_len=2000, batchsize=32, device=None):\n",
    "        \"\"\"\n",
    "        InfoNet互信息估计器\n",
    "\n",
    "        :param model: 已加载的InfoNet模型（手动加载后传入）\n",
    "        :param proj_num: 投影数量（高维互信息估计用）\n",
    "        :param seq_len: 采样点数\n",
    "        :param batchsize: 批大小\n",
    "        :param device: 设备（'cuda'或'cpu'），默认自动检测\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.proj_num = proj_num\n",
    "        self.seq_len = seq_len\n",
    "        self.batchsize = batchsize\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def estimate_1d(self, x, y):\n",
    "        \"\"\"\n",
    "        一维互信息估计\n",
    "        :param x: shape [N,]\n",
    "        :param y: shape [N,]\n",
    "        :return: 互信息估计值\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        x = rankdata(x) / len(x)\n",
    "        y = rankdata(y) / len(y)\n",
    "        batch = (\n",
    "            torch.stack(\n",
    "                (\n",
    "                    torch.tensor(x, dtype=torch.float32),\n",
    "                    torch.tensor(y, dtype=torch.float32),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            mi_lb = model(batch)\n",
    "        return mi_lb.squeeze().cpu().numpy()\n",
    "\n",
    "    def estimate_highd(self, sample_x, sample_y):\n",
    "        \"\"\"\n",
    "        高维互信息估计（Sliced MI）\n",
    "        :param sample_x: shape [N, dx]\n",
    "        :param sample_y: shape [N, dy]\n",
    "        :return: 互信息估计值\n",
    "        \"\"\"\n",
    "        dx = sample_x.shape[1]\n",
    "        dy = sample_y.shape[1]\n",
    "        results = []\n",
    "        for i in range(self.proj_num // self.batchsize):\n",
    "            batch = np.zeros((self.batchsize, self.seq_len, 2))\n",
    "            for j in range(self.batchsize):\n",
    "                theta = np.random.randn(dx)\n",
    "                phi = np.random.randn(dy)\n",
    "                x_proj = np.dot(sample_x, theta)\n",
    "                y_proj = np.dot(sample_y, phi)\n",
    "                x_proj = rankdata(x_proj) / self.seq_len\n",
    "                y_proj = rankdata(y_proj) / self.seq_len\n",
    "                xy = np.column_stack((x_proj, y_proj))\n",
    "                batch[j, :, :] = xy\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                mi_lb = self.model(batch_tensor)\n",
    "                mean_infer1 = torch.mean(mi_lb).cpu().numpy()\n",
    "            results.append(mean_infer1)\n",
    "        return float(np.mean(np.array(results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d86e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一维互信息估计: 0.019440778\n",
      "高维互信息估计: 0.039300866425037384\n"
     ]
    }
   ],
   "source": [
    "from infonet.infer import load_model\n",
    "model = load_model('../../configs/infonet/config.yaml', '../../data/checkpoint/infonet_cp/model_5000_32_1000-720--0.16.pt')\n",
    "estimator = InfoNetEstimator(model, proj_num=512, seq_len=1000, batchsize=16)\n",
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n",
    "mi_1d = estimator.estimate_1d(x, y)\n",
    "print(\"一维互信息估计:\", mi_1d)\n",
    "X = np.random.randn(1000, 10)\n",
    "Y = np.random.randn(1000, 10)\n",
    "mi_hd = estimator.estimate_highd(X, Y)\n",
    "print(\"高维互信息估计:\", mi_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c45eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# paramter法查看model是否是cuda设备\n",
    "def is_model_cuda(model):\n",
    "    return next(model.parameters()).is_cuda\n",
    "print(\"Model is on CUDA:\", is_model_cuda(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18887403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import light\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "class LeNet(light.ModernModule):\n",
    "    def __init__(self, config: light.ModelConfig):\n",
    "        super().__init__(config)\n",
    "        self._is_classification = True\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(\n",
    "                16,\n",
    "                kernel_size=5,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LazyLinear(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LazyLinear(config.output_size),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, y_hat, y) -> Tensor:\n",
    "        return nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "    def predict(self, x: Tensor):\n",
    "        return self.net(x).argmax(dim=1)\n",
    "\n",
    "    def predict_class(self, x: Tensor) -> Tensor:\n",
    "        return self.predict(x).item()\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # 评估模型性能\n",
    "        # 整体正确率 - FP - FN - F1score - AUC\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.net(X)\n",
    "            loss = self.compute_loss(y_hat, y)\n",
    "            predicted_class = self.predict(X)\n",
    "            accuracy = (predicted_class == y).float().mean().item()\n",
    "            return {\n",
    "                \"loss\": loss.item(),\n",
    "                \"accuracy\": accuracy,\n",
    "                \"predicted_class\": predicted_class,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f06c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset on CUDA: False\n",
      "Test dataset on CUDA: False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "datapath = \"../../data/\"\n",
    "\n",
    "# 数据加载\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=datapath, train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=datapath, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# 查看数据集是否在CUDA上运行\n",
    "def is_dataset_on_cuda(dataset):\n",
    "    return next(iter(dataset))[0].is_cuda\n",
    "print(\"Train dataset on CUDA:\", is_dataset_on_cuda(train_loader))\n",
    "print(\"Test dataset on CUDA:\", is_dataset_on_cuda(test_loader))\n",
    "# 转移数据到CUDA设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9650cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 互信息计算回调\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TestCallback(pl.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # 在每个训练周期结束时计算互信息\n",
    "        # 写入到 ./logs/infometrics.txt\n",
    "        # 没有自动创建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e5351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | net  | Sequential | 0      | train\n",
      "--------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in dataset: 10\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:15<00:00, 62.12it/s, v_num=2, train_loss=2.290, train_acc=0.0938, val_loss=2.300, val_acc=0.113]"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/infometrics.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m trainer_lenet = light.TrainerFactory.basic(\n\u001b[32m     17\u001b[39m     max_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m trainer_lenet.add_callback(TestCallback())\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mtrainer_lenet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLeNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\src\\utils\\model\\light.py:738\u001b[39m, in \u001b[36mModernTrainer.fit\u001b[39m\u001b[34m(self, model, train_loader, val_loader)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = pl.Trainer(\n\u001b[32m    733\u001b[39m     callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m    734\u001b[39m     **\u001b[38;5;28mself\u001b[39m.trainer_kwargs\n\u001b[32m    735\u001b[39m )\n\u001b[32m    737\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:217\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance()\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:468\u001b[39m, in \u001b[36m_FitLoop.on_advance_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch_progress.increment_processed()\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# call train epoch end hooks\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_epoch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m call._call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    470\u001b[39m call._call_callback_hooks(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m, monitoring_callbacks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:227\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    231\u001b[39m     pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTestCallback.on_train_epoch_end\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, pl_module):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 在每个训练周期结束时计算互信息\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# 写入到 ./logs/infometrics.txt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs/infometrics.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m         f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.current_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: 计算互信息...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     X, y = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Src\\SevFoxie\\SevFoxie\\Study\\2025Summer\\DeepLearning\\code\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './logs/infometrics.txt'"
     ]
    }
   ],
   "source": [
    "# 从数据集读取class\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Number of classes in dataset:\", num_classes)\n",
    "\n",
    "config = light.ModelConfig(\n",
    "    lr=0.001,  # 较小的学习率\n",
    "    num_hiddens=256,\n",
    "    output_size=num_classes,\n",
    "    optimizer=\"sgd\",  \n",
    "    weight_decay=1e-4,  # 正则化\n",
    "    momentum=0.9,  # 动量\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer_lenet = light.TrainerFactory.basic(\n",
    "    max_epochs=10,\n",
    ")\n",
    "trainer_lenet.add_callback(TestCallback())\n",
    "trainer_lenet.fit(\n",
    "    model=LeNet(config),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
