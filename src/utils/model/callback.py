import time
import threading
from contextlib import contextmanager
import numpy as np
import torch
import torch.nn as nn
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback
from livelossplot import PlotLosses

import csv
import os
from datetime import datetime
import pandas as pd


class DisplayManager:
    """æ˜¾ç¤ºç®¡ç†å™¨ - åè°ƒå¤šä¸ªå›è°ƒçš„è¾“å‡ºæ˜¾ç¤º"""
    
    def __init__(self):
        self.is_plotting = False
        self.pending_messages = []
        self.lock = threading.Lock()
        
    @contextmanager
    def exclusive_display(self, callback_name=""):
        """ç‹¬å æ˜¾ç¤ºä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        with self.lock:
            self.is_plotting = True
            print(f"\nğŸ“Š {callback_name} æ˜¾ç¤ºå¼€å§‹...")
            try:
                yield
            finally:
                print(f"âœ… {callback_name} æ˜¾ç¤ºå®Œæˆ\n")
                self.is_plotting = False
                self._flush_pending_messages()
    
    def safe_print(self, message, force=False):
        """å®‰å…¨æ‰“å° - å¦‚æœæ­£åœ¨ç»˜å›¾åˆ™ç¼“å­˜æ¶ˆæ¯"""
        with self.lock:
            if self.is_plotting and not force:
                self.pending_messages.append(message)
            else:
                print(message)
    
    def _flush_pending_messages(self):
        """è¾“å‡ºç¼“å­˜çš„æ¶ˆæ¯"""
        if self.pending_messages:
            print("ğŸ“‹ ç¼“å­˜çš„æ¶ˆæ¯:")
            for msg in self.pending_messages:
                print(msg)
            self.pending_messages.clear()


# å…¨å±€æ˜¾ç¤ºç®¡ç†å™¨å®ä¾‹
_display_manager = DisplayManager()

class FilePersistentMutualInformationCallback(Callback):
    """æ–‡ä»¶æŒä¹…åŒ–MIå›è°ƒ - åŸºäºInfoNetå®˜æ–¹å®ç°"""
    
    def __init__(self, mi_model, eval_loader, eval_every_n_epochs=1, 
                 save_dir="./mi_results", experiment_name=None,
                 seq_len=2000, proj_num=1024, batchsize=32):
        super().__init__()
        self.mi_model = mi_model  # InfoNetæ¨¡å‹
        self.eval_loader = eval_loader
        self.eval_every_n_epochs = eval_every_n_epochs
        
        # InfoNetå®˜æ–¹è¶…å‚æ•°
        self.seq_len = seq_len  # å®˜æ–¹é»˜è®¤2000æ ·æœ¬ç”¨äºä¼°è®¡
        self.proj_num = proj_num  # å®˜æ–¹é»˜è®¤1024ä¸ªéšæœºæŠ•å½±
        self.batchsize = batchsize  # å®˜æ–¹é»˜è®¤32ä¸ªä¸€ç»´å¯¹åŒæ—¶ä¼°è®¡
        
        # æ–‡ä»¶å­˜å‚¨é…ç½®
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # ç”Ÿæˆå®éªŒæ ‡è¯†
        if experiment_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"infonet_mi_experiment_{timestamp}"
        self.experiment_name = experiment_name
        
        # CSVæ–‡ä»¶è·¯å¾„
        self.csv_path = os.path.join(save_dir, f"{experiment_name}_mi_results.csv")
        self.summary_path = os.path.join(save_dir, f"{experiment_name}_summary.txt")
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        self._init_csv_file()
        
        # æ˜¾ç¤ºæ§åˆ¶
        self.show_summary_every_n_epochs = 5
        
        # éªŒè¯InfoNetæ¨¡å‹
        if self.mi_model is None:
            raise ValueError("âŒ å¿…é¡»æä¾›æœ‰æ•ˆçš„InfoNetæ¨¡å‹")
        
        print(f"âœ… MIç»“æœå°†ä¿å­˜åˆ°: {self.csv_path}")
        print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š: {self.summary_path}")
        print(f"ğŸ”¬ ä½¿ç”¨InfoNetè¿›è¡ŒMIä¼°è®¡ (seq_len={seq_len}, proj_num={proj_num})")
    
    def _init_csv_file(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        fieldnames = ['epoch', 'timestamp', 'layer', 'I_XT', 'I_TY', 'status', 'method', 'sample_size', 'proj_num']
        
        with open(self.csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
    
    def on_train_epoch_end(self, trainer, pl_module):
        """MIåˆ†æ + æ–‡ä»¶å­˜å‚¨"""
        current_epoch = trainer.current_epoch
        
        if current_epoch % self.eval_every_n_epochs == 0:
            print(f"ğŸ” Epoch {current_epoch} - InfoNet MIåˆ†æä¸­...")
            
            # æ‰§è¡ŒMIåˆ†æ
            results = self._perform_infonet_mi_analysis(trainer, pl_module, current_epoch)
            
            # ä¿å­˜åˆ°CSV
            self._save_results_to_csv(current_epoch, results)
            
            # ç®€åŒ–çš„å³æ—¶åé¦ˆ
            if results:
                valid_results = [r for r in results if r.get('I_XT') is not None]
                if valid_results:
                    avg_ixt = np.mean([r['I_XT'] for r in valid_results])
                    avg_ity = np.mean([r['I_TY'] for r in valid_results])
                    print(f"âœ… Epoch {current_epoch} - å¹³å‡MI: I(X;T)={avg_ixt:.4f}, I(T;Y)={avg_ity:.4f}")
            
            # å‘¨æœŸæ€§æ˜¾ç¤ºè¯¦ç»†æ±‡æ€»
            if current_epoch % self.show_summary_every_n_epochs == 0 and current_epoch > 0:
                self._show_periodic_summary()
    
    def _perform_infonet_mi_analysis(self, trainer, pl_module, current_epoch):
        """æ‰§è¡ŒåŸºäºInfoNetçš„MIåˆ†æ"""
        try:
            mi_analyzer = self._create_infonet_analyzer(pl_module)
            
            # æ”¶é›†æ•°æ® - æŒ‰å®˜æ–¹æ–¹å¼æ”¶é›†è¶³å¤Ÿæ ·æœ¬
            X_list, y_list = [], []
            total_samples = 0
            for i, (X, y) in enumerate(self.eval_loader):
                X_list.append(X)
                y_list.append(y)
                total_samples += X.size(0)
                # æ”¶é›†è¶³å¤Ÿæ ·æœ¬æˆ–è¾¾åˆ°æœ€å¤§batchæ•°
                if total_samples >= self.seq_len or i >= 19:  # æœ€å¤š20ä¸ªbatch
                    break
            
            if not X_list:
                return []
            
            X_combined = torch.cat(X_list, dim=0)
            y_combined = torch.cat(y_list, dim=0)
            
            # å¦‚æœæ ·æœ¬è¿‡å¤šï¼Œéšæœºé‡‡æ ·åˆ°seq_len
            if len(X_combined) > self.seq_len:
                indices = torch.randperm(len(X_combined))[:self.seq_len]
                X_combined = X_combined[indices]
                y_combined = y_combined[indices]
            
            # åˆ†æå„å±‚
            results = []
            layers = ['conv1', 'conv2', 'fc1', 'fc2', 'fc3']
            
            for layer_name in layers:
                try:
                    layer_results = mi_analyzer.analyze_layer_with_infonet(
                        X_combined, y_combined, layer_name
                    )
                    
                    result_record = {
                        'layer': layer_name,
                        'I_XT': layer_results.get('I(X;T)', None),
                        'I_TY': layer_results.get('I(T;Y)', None),
                        'status': 'success' if layer_results else 'failed',
                        'method': 'infonet_official',
                        'sample_size': layer_results.get('sample_size', 0),
                        'proj_num': layer_results.get('proj_num', 0)
                    }
                    results.append(result_record)
                    
                except Exception as e:
                    print(f"âš ï¸ å±‚ {layer_name} åˆ†æå¤±è´¥: {str(e)[:100]}")
                    results.append({
                        'layer': layer_name,
                        'I_XT': None,
                        'I_TY': None,
                        'status': f'error: {str(e)[:50]}',
                        'method': 'infonet_official',
                        'sample_size': 0,
                        'proj_num': 0
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ InfoNet MIåˆ†æå‡ºé”™: {e}")
            return []
    
    def _create_infonet_analyzer(self, trained_model):
        """åˆ›å»ºåŸºäºInfoNetå®˜æ–¹å®ç°çš„MIåˆ†æå™¨"""
        from infonet.infer import estimate_mi, compute_smi_mean
        from scipy.stats import rankdata
        
        class InfoNetOfficialMIAnalyzer(nn.Module):
            def __init__(self, source_model, infonet_model, seq_len, proj_num, batchsize):
                super().__init__()
                self.net = source_model.net
                self.mi_model = infonet_model
                self.seq_len = seq_len
                self.proj_num = proj_num
                self.batchsize = batchsize
                self.activations = {}
                self._register_hooks()
            
            def _register_hooks(self):
                """æ³¨å†Œå‰å‘ä¼ æ’­é’©å­"""
                def hook_fn(name):
                    def hook(module, input, output):
                        self.activations[name] = output.detach()
                    return hook
                
                try:
                    self.net[0].register_forward_hook(hook_fn('conv1'))
                    self.net[3].register_forward_hook(hook_fn('conv2'))
                    self.net[7].register_forward_hook(hook_fn('fc1'))
                    self.net[9].register_forward_hook(hook_fn('fc2'))
                    self.net[11].register_forward_hook(hook_fn('fc3'))
                except Exception as e:
                    print(f"âŒ Hookæ³¨å†Œå¤±è´¥: {e}")
            
            def forward(self, x):
                return self.net(x)
            
            def analyze_layer_with_infonet(self, X, y, target_layer):
                """ä½¿ç”¨InfoNetå®˜æ–¹æ–¹æ³•åˆ†æç‰¹å®šå±‚çš„äº’ä¿¡æ¯"""
                self.eval()
                
                # å‰å‘ä¼ æ’­è·å–æ¿€æ´»
                with torch.no_grad():
                    _ = self.forward(X)
                
                if target_layer not in self.activations:
                    print(f"âŒ æœªæ‰¾åˆ°å±‚ {target_layer} çš„æ¿€æ´»")
                    return {}
                
                T = self.activations[target_layer]
                
                # ä½¿ç”¨å®˜æ–¹æ–¹æ³•è¿›è¡ŒMIä¼°è®¡
                return self._official_mutual_information(X, T, y)
            
            def _official_mutual_information(self, X, T, y):
                """ä½¿ç”¨InfoNetå®˜æ–¹æ–¹æ³•ä¼°è®¡äº’ä¿¡æ¯"""
                try:
                    results = {'sample_size': len(X)}
                    
                    # æ•°æ®é¢„å¤„ç† - è½¬æ¢ä¸ºnumpy
                    X_np = X.view(X.size(0), -1).cpu().numpy()
                    T_np = T.view(T.size(0), -1).cpu().numpy()
                    y_np = y.cpu().numpy()
                    
                    # I(X;T)ä¼°è®¡ - æŒ‰å®˜æ–¹ç»´åº¦åˆ¤æ–­é€»è¾‘
                    if X_np.shape[1] <= 2 and T_np.shape[1] <= 2:
                        # ä½ç»´æƒ…å†µï¼šç›´æ¥ä½¿ç”¨estimate_mi (å®˜æ–¹ç¤ºä¾‹ä¸­çš„1ç»´æƒ…å†µ)
                        results['I(X;T)'] = self._estimate_1d_mi(X_np, T_np, 'I(X;T)')
                        results['proj_num'] = 1
                    else:
                        # é«˜ç»´æƒ…å†µï¼šä½¿ç”¨compute_smi_mean (å®˜æ–¹é«˜ç»´ç¤ºä¾‹)
                        results['I(X;T)'] = self._compute_smi_mean_official(X_np, T_np, 'I(X;T)')
                        results['proj_num'] = self.proj_num
                    
                    # I(T;Y)ä¼°è®¡ - åˆ†ç±»ä»»åŠ¡ç‰¹æ®Šå¤„ç†
                    y_processed = self._process_labels_official(y_np)
                    if T_np.shape[1] <= 2:
                        results['I(T;Y)'] = self._estimate_1d_mi_classification(T_np, y_processed, 'I(T;Y)')
                    else:
                        results['I(T;Y)'] = self._compute_smi_classification(T_np, y_processed, 'I(T;Y)')
                    
                    return results
                    
                except Exception as e:
                    print(f"âŒ InfoNetå®˜æ–¹MIä¼°è®¡å¤±è´¥: {e}")
                    return {}
            
            def _estimate_1d_mi(self, X_data, T_data, mi_type):
                """1ç»´MIä¼°è®¡ - å®Œå…¨æŒ‰å®˜æ–¹ç¤ºä¾‹"""
                try:
                    # é€‰æ‹©ä»£è¡¨æ€§ç‰¹å¾ï¼ˆå¦‚æœå¤šç»´åˆ™å–ç¬¬ä¸€ç»´æˆ–ä¸»æˆåˆ†ï¼‰
                    if X_data.shape[1] > 1:
                        x_feature = X_data[:, 0]  # ç®€å•å–ç¬¬ä¸€ç»´ï¼Œæˆ–å¯ç”¨PCA
                    else:
                        x_feature = X_data.flatten()
                    
                    if T_data.shape[1] > 1:
                        t_feature = T_data[:, 0]
                    else:
                        t_feature = T_data.flatten()
                    
                    # å®˜æ–¹é¢„å¤„ç†æ–¹å¼
                    x_ranked = rankdata(x_feature) / len(x_feature)
                    t_ranked = rankdata(t_feature) / len(t_feature)
                    
                    # ä½¿ç”¨estimate_miå‡½æ•°
                    mi_est = estimate_mi(self.mi_model, x_ranked, t_ranked)
                    if isinstance(mi_est, torch.Tensor):
                        mi_est = mi_est.item()
                    
                    print(f"ğŸ“Š {mi_type} 1ç»´ä¼°è®¡: {mi_est:.4f}")
                    return mi_est
                    
                except Exception as e:
                    print(f"âŒ 1ç»´MIä¼°è®¡å¤±è´¥: {e}")
                    return None
            
            def _compute_smi_mean_official(self, X_data, T_data, mi_type):
                """é«˜ç»´SMIä¼°è®¡ - å®Œå…¨æŒ‰å®˜æ–¹compute_smi_meanå®ç°"""
                try:
                    # ç›´æ¥è°ƒç”¨å®˜æ–¹å‡½æ•°
                    smi_result = compute_smi_mean(
                        sample_x=X_data,
                        sample_y=T_data,
                        model=self.mi_model,
                        proj_num=self.proj_num,
                        seq_len=self.seq_len,
                        batchsize=self.batchsize
                    )
                    
                    print(f"ğŸ“Š {mi_type} SMIä¼°è®¡: {smi_result:.4f} (proj_num={self.proj_num})")
                    return smi_result
                    
                except Exception as e:
                    print(f"âŒ SMIä¼°è®¡å¤±è´¥ï¼Œå›é€€åˆ°ç®€åŒ–ç‰ˆæœ¬: {e}")
                    # å›é€€åˆ°ç®€åŒ–çš„SMIå®ç°
                    return self._fallback_smi_estimation(X_data, T_data, mi_type)
            
            def _fallback_smi_estimation(self, X_data, T_data, mi_type):
                """ç®€åŒ–ç‰ˆSMIä¼°è®¡ - å½“å®˜æ–¹å‡½æ•°ä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ¡ˆ"""
                try:
                    # ä½¿ç”¨è¾ƒå°‘çš„æŠ•å½±æ¬¡æ•°è¿›è¡Œå›é€€
                    n_projections = min(64, self.proj_num // 16)  # å‡å°‘æŠ•å½±æ•°
                    mi_estimates = []
                    
                    for _ in range(n_projections):
                        # éšæœºæŠ•å½±
                        if X_data.shape[1] > 1:
                            proj_x = np.random.randn(X_data.shape[1])
                            proj_x = proj_x / np.linalg.norm(proj_x)
                            x_proj = X_data @ proj_x
                        else:
                            x_proj = X_data.flatten()
                        
                        if T_data.shape[1] > 1:
                            proj_t = np.random.randn(T_data.shape[1])
                            proj_t = proj_t / np.linalg.norm(proj_t)
                            t_proj = T_data @ proj_t
                        else:
                            t_proj = T_data.flatten()
                        
                        # æ ‡å‡†åŒ–å’Œä¼°è®¡
                        if np.std(x_proj) > 1e-6 and np.std(t_proj) > 1e-6:
                            x_ranked = rankdata(x_proj) / len(x_proj)
                            t_ranked = rankdata(t_proj) / len(t_proj)
                            
                            mi_est = estimate_mi(self.mi_model, x_ranked, t_ranked)
                            if isinstance(mi_est, torch.Tensor):
                                mi_est = mi_est.item()
                            
                            if 0 <= mi_est <= 15:
                                mi_estimates.append(mi_est)
                    
                    if mi_estimates:
                        avg_mi = np.mean(mi_estimates)
                        print(f"ğŸ“Š {mi_type} å›é€€SMIä¼°è®¡: {avg_mi:.4f} (proj_num={n_projections})")
                        return avg_mi
                    else:
                        return None
                        
                except Exception as e:
                    print(f"âŒ å›é€€SMIä¼°è®¡ä¹Ÿå¤±è´¥: {e}")
                    return None
            
            def _estimate_1d_mi_classification(self, T_data, y_processed, mi_type):
                """1ç»´åˆ†ç±»MIä¼°è®¡"""
                try:
                    # é€‰æ‹©ç‰¹å¾
                    if T_data.shape[1] > 1:
                        t_feature = T_data[:, 0]  # ç®€å•é€‰æ‹©ç¬¬ä¸€ç»´
                    else:
                        t_feature = T_data.flatten()
                    
                    # æ ‡å‡†åŒ–
                    if np.std(t_feature) > 1e-6 and np.std(y_processed) > 1e-6:
                        t_ranked = rankdata(t_feature) / len(t_feature)
                        y_ranked = rankdata(y_processed) / len(y_processed)
                        
                        mi_est = estimate_mi(self.mi_model, t_ranked, y_ranked)
                        if isinstance(mi_est, torch.Tensor):
                            mi_est = mi_est.item()
                        
                        print(f"ğŸ“Š {mi_type} 1ç»´åˆ†ç±»ä¼°è®¡: {mi_est:.4f}")
                        return mi_est
                    
                    return None
                    
                except Exception as e:
                    print(f"âŒ 1ç»´åˆ†ç±»MIä¼°è®¡å¤±è´¥: {e}")
                    return None
            
            def _compute_smi_classification(self, T_data, y_processed, mi_type):
                """é«˜ç»´åˆ†ç±»SMIä¼°è®¡"""
                try:
                    # å°†y_processedæ‰©å±•ä¸ºäºŒç»´ä»¥ç¬¦åˆcompute_smi_meanæ¥å£
                    y_expanded = y_processed.reshape(-1, 1)
                    
                    # ä½¿ç”¨å®˜æ–¹SMIæ–¹æ³•
                    smi_result = compute_smi_mean(
                        sample_x=T_data,
                        sample_y=y_expanded,
                        model=self.mi_model,
                        proj_num=self.proj_num // 2,  # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨è¾ƒå°‘æŠ•å½±
                        seq_len=self.seq_len,
                        batchsize=self.batchsize
                    )
                    
                    print(f"ğŸ“Š {mi_type} åˆ†ç±»SMIä¼°è®¡: {smi_result:.4f}")
                    return smi_result
                    
                except Exception as e:
                    print(f"âŒ åˆ†ç±»SMIä¼°è®¡å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ: {e}")
                    return self._fallback_classification_smi(T_data, y_processed, mi_type)
            
            def _fallback_classification_smi(self, T_data, y_processed, mi_type):
                """åˆ†ç±»ä»»åŠ¡çš„å›é€€SMIä¼°è®¡"""
                try:
                    n_projections = min(32, self.proj_num // 32)
                    mi_estimates = []
                    
                    for _ in range(n_projections):
                        if T_data.shape[1] > 1:
                            proj_t = np.random.randn(T_data.shape[1])
                            proj_t = proj_t / np.linalg.norm(proj_t)
                            t_proj = T_data @ proj_t
                        else:
                            t_proj = T_data.flatten()
                        
                        if np.std(t_proj) > 1e-6 and np.std(y_processed) > 1e-6:
                            t_ranked = rankdata(t_proj) / len(t_proj)
                            y_ranked = rankdata(y_processed) / len(y_processed)
                            
                            mi_est = estimate_mi(self.mi_model, t_ranked, y_ranked)
                            if isinstance(mi_est, torch.Tensor):
                                mi_est = mi_est.item()
                            
                            if 0 <= mi_est <= 15:
                                mi_estimates.append(mi_est)
                    
                    if mi_estimates:
                        avg_mi = np.mean(mi_estimates)
                        print(f"ğŸ“Š {mi_type} å›é€€åˆ†ç±»SMI: {avg_mi:.4f}")
                        return avg_mi
                    else:
                        return None
                        
                except Exception as e:
                    print(f"âŒ å›é€€åˆ†ç±»SMIå¤±è´¥: {e}")
                    return None
            
            def _process_labels_official(self, y_np):
                """å®˜æ–¹é£æ ¼çš„æ ‡ç­¾å¤„ç†"""
                unique_classes = np.unique(y_np)
                
                if len(unique_classes) <= 1:
                    return np.zeros_like(y_np, dtype=float)
                
                # ç®€å•çš„æ ‡ç­¾åˆ°è¿ç»­å€¼æ˜ å°„
                y_processed = np.zeros_like(y_np, dtype=float)
                for i, cls in enumerate(unique_classes):
                    mask = y_np == cls
                    y_processed[mask] = i / (len(unique_classes) - 1)
                
                # æ·»åŠ å°é‡å™ªå£°ä½¿å…¶è¿ç»­
                noise_scale = 0.01 / len(unique_classes)
                y_processed += np.random.normal(0, noise_scale, size=y_processed.shape)
                
                return y_processed
        
        return InfoNetOfficialMIAnalyzer(
            trained_model, 
            self.mi_model, 
            self.seq_len, 
            self.proj_num, 
            self.batchsize
        )
    
    def _save_results_to_csv(self, epoch, results):
        """ä¿å­˜ç»“æœåˆ°CSV"""
        timestamp = datetime.now().isoformat()
        
        with open(self.csv_path, 'a', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['epoch', 'timestamp', 'layer', 'I_XT', 'I_TY', 'status', 'method', 'sample_size', 'proj_num']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            for result in results:
                writer.writerow({
                    'epoch': epoch,
                    'timestamp': timestamp,
                    'layer': result['layer'],
                    'I_XT': result['I_XT'],
                    'I_TY': result['I_TY'],
                    'status': result['status'],
                    'method': result.get('method', 'infonet_official'),
                    'sample_size': result.get('sample_size', 0),
                    'proj_num': result.get('proj_num', 0)
                })
    
    def _show_periodic_summary(self):
        """æ˜¾ç¤ºå‘¨æœŸæ€§æ±‡æ€»æŠ¥å‘Š"""
        try:
            if os.path.exists(self.csv_path):
                df = pd.read_csv(self.csv_path)
                if not df.empty:
                    print(f"\nğŸ“Š {self.experiment_name} - å‘¨æœŸæ€§æ±‡æ€»æŠ¥å‘Š")
                    print("=" * 60)
                    
                    # æŒ‰å±‚æ˜¾ç¤ºæœ€æ–°ç»“æœ
                    latest_epoch = df['epoch'].max()
                    latest_data = df[df['epoch'] == latest_epoch]
                    
                    for _, row in latest_data.iterrows():
                        if row['I_XT'] is not None and row['I_TY'] is not None:
                            proj_info = f"(proj_num={row.get('proj_num', 'N/A')})" if row.get('proj_num') else ""
                            print(f"å±‚ {row['layer']}: I(X;T)={row['I_XT']:.4f}, I(T;Y)={row['I_TY']:.4f} {proj_info}")
                    
                    print("=" * 60)
                else:
                    print("ğŸ“‹ æš‚æ— MIæ•°æ®å¯æ˜¾ç¤º")
            else:
                print("ğŸ“‹ MIç»“æœæ–‡ä»¶å°šæœªåˆ›å»º")
        except Exception as e:
            print(f"âš ï¸ æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

class EnhancedHighFrequencyLiveLossPlotCallback(pl.Callback):
    """å¢å¼ºç‰ˆLiveLosså›è°ƒ - é›†æˆæ˜¾ç¤ºç®¡ç†"""
    
    def __init__(self, update_every_n_batches=10, display_manager=None):
        super().__init__()
        self.liveloss = PlotLosses()
        self.update_every_n_batches = update_every_n_batches
        self.batch_count = 0
        self.display_manager = display_manager or _display_manager
        
        # ç”¨äºç´¯ç§¯æŒ‡æ ‡
        self.train_loss_accumulator = []
        self.current_epoch = 0
        
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """æ¯ä¸ªè®­ç»ƒbatchç»“æŸåè°ƒç”¨"""
        self.batch_count += 1
        
        # ç´¯ç§¯å½“å‰batchçš„æŸå¤±
        if outputs is not None:
            if isinstance(outputs, dict) and 'loss' in outputs:
                self.train_loss_accumulator.append(outputs['loss'].item())
            elif hasattr(outputs, 'item'):
                self.train_loss_accumulator.append(outputs.item())
            elif torch.is_tensor(outputs):
                self.train_loss_accumulator.append(outputs.item())
        
        # æ¯éš” N ä¸ª batch æ›´æ–°ä¸€æ¬¡å›¾è¡¨
        if self.batch_count % self.update_every_n_batches == 0:
            self._update_plot(trainer, pl_module)
    
    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯ç»“æŸåä¹Ÿæ›´æ–°ä¸€æ¬¡"""
        self._update_plot(trainer, pl_module)
        self.current_epoch += 1
        self.train_loss_accumulator = []
    
    def _update_plot(self, trainer, pl_module):
        """å—ç®¡ç†çš„å›¾è¡¨æ›´æ–°"""
        # å¦‚æœMIæ­£åœ¨åˆ†æï¼Œåˆ™è·³è¿‡è¿™æ¬¡æ›´æ–°
        if self.display_manager.is_plotting:
            return
        
        logs = {}
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        if self.train_loss_accumulator:
            avg_train_loss = sum(self.train_loss_accumulator) / len(self.train_loss_accumulator)
            logs['log loss'] = avg_train_loss
        
        # è·å–æœ€æ–°çš„éªŒè¯æŒ‡æ ‡
        if 'val_loss' in trainer.callback_metrics:
            logs['val_log loss'] = trainer.callback_metrics['val_loss'].item()
        if 'val_acc' in trainer.callback_metrics:
            logs['val_accuracy'] = trainer.callback_metrics['val_acc'].item()
        if 'train_acc' in trainer.callback_metrics:
            logs['accuracy'] = trainer.callback_metrics['train_acc'].item()
        
        # æ›´æ–°å›¾è¡¨
        if logs:
            self.liveloss.update(logs)
            self.liveloss.send()