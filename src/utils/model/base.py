from pydantic import BaseModel, Field
from livelossplot import PlotLosses
import torch
from torch import device, nn
import platform
import psutil
import time

from typing import List, Optional, Tuple, Union, TYPE_CHECKING
import collections
# import time

import matplotlib
import matplotlib.pyplot as plt
from IPython import display
import numpy as np

if TYPE_CHECKING:
    from typing import TYPE_CHECKING


class BoardConfig(BaseModel):
    """Configuration for the ProgressBoard using Pydantic."""

    xlabel: Optional[str] = None
    ylabel: Optional[str] = None
    xlim: Optional[Tuple[float, float]] = None
    ylim: Optional[Tuple[float, float]] = None
    xscale: str = "linear"
    yscale: str = "linear"
    ls: List[str] = Field(default_factory=lambda: ["-", "--", "-.", ":"])
    colors: List[str] = Field(default_factory=lambda: ["C0", "C1", "C2", "C3"])
    figsize: Tuple[float, float] = (4.5, 3.5)

    # Pydantic v2 ‰∏≠ÔºåËøôÊòØ‰∏Ä‰∏™Â•Ω‰π†ÊÉØÔºåÂÖÅËÆ∏Âú®Ê®°ÂûãÂ§ñÈÉ®‰øÆÊîπÂ≠óÊÆµ
    class Config:
        validate_assignment = True


class ProgressBoard:
    """A modern ProgressBoard that uses a Pydantic config object."""

    def __init__(self, config: Optional[BoardConfig] = None):
        """Initializes the board with a configuration object."""
        # Â¶ÇÊûúÊ≤°ÊúâÊèê‰æõÈÖçÁΩÆÔºåÂ∞±‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ
        self.config = config or BoardConfig()

        # ÂàõÂª∫ÂõæÂΩ¢ÂíåÂùêÊ†áËΩ¥
        self.fig, self.axes = plt.subplots(figsize=self.config.figsize)

        # ÂÜÖÈÉ®Êï∞ÊçÆÂíåËÆ°Êï∞Âô®Â≠òÂÇ®
        self.data = collections.OrderedDict()
        self.drawn = collections.OrderedDict()

    def _draw_figure(self):
        """The core drawing logic."""
        display.clear_output(wait=True)
        self.axes.cla()

        for i, (label, (xs, ys)) in enumerate(self.data.items()):
            self.axes.plot(
                xs,
                ys,
                linestyle=self.config.ls[i % len(self.config.ls)],
                color=self.config.colors[i % len(self.config.colors)],
                label=label,
            )

        # ‰ªé config ÂØπË±°‰∏≠ËØªÂèñÈÖçÁΩÆÊù•ËÆæÁΩÆÂõæË°®
        if self.config.xlabel:
            self.axes.set_xlabel(self.config.xlabel)
        if self.config.ylabel:
            self.axes.set_ylabel(self.config.ylabel)
        self.axes.set_xscale(self.config.xscale)
        self.axes.set_yscale(self.config.yscale)
        if self.config.xlim:
            self.axes.set_xlim(self.config.xlim)
        if self.config.ylim:
            self.axes.set_ylim(self.config.ylim)
        self.axes.grid()
        self.axes.legend()

        display.display(self.fig)

    def draw(
        self,
        x: Union[float, np.floating],
        y: Union[float, np.floating],
        label: str,
        every_n: int = 1,
    ):
        """Add a data point and redraw the figure."""
        if label not in self.data:
            self.data[label] = ([], [])
            self.drawn[label] = 0

        self.data[label][0].append(float(x))
        self.data[label][1].append(float(y))
        self.drawn[label] += 1

        if self.drawn[label] % every_n == 0:
            self._draw_figure()


class ModelConfig(BaseModel):
    """Configuration for the Model."""

    lr: float = 0.01
    num_hiddens: int = 256
    output_size: int = 10
    # ËøòÂèØ‰ª•ÂåÖÂê´ÂÖ∂‰ªñÈÖçÁΩÆÔºåÂ¶Ç dropout_prob, weight_decayÁ≠â
    self_device: Optional[str] = None  # Êñ∞Â¢ûÔºöÊåáÂÆöËÆæÂ§áÔºåÂ¶Ç "cuda:1"
    
    # ÈùôÊÄÅÊñπÊ≥ïÂ±ïÁ§∫ÊâÄÊúâÂèØÁî®gpuËÆæÂ§áÂèäÂÖ∂ÈÄâÊã©id
    @staticmethod
    def available_devices() -> List[str]:
        """Returns a list of available devices."""
        if torch.cuda.is_available():
            return [f"cuda:{i}" for i in range(torch.cuda.device_count())]
        else:
            return ["cpu"]
class ModernModule(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.self_device = self._resolve_device()
        self.to(self.self_device)

    def _resolve_device(self) -> torch.device:
        if self.config.self_device:
            return torch.device(self.config.self_device)
        return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    def set_device(self, device_str: str):
        self.config.self_device = device_str
        self.self_device = torch.device(device_str)
        self.to(self.self_device)

    def forward(self, X):
        """Default forward pass."""
        if not hasattr(self, "net"):
            raise NotImplementedError(
                "You must define 'self.net' in your model's __init__."
            )
        if not callable(self.net):
            raise TypeError(
                "self.net must be a callable (e.g., nn.Module), got a Tensor instead. "
                "Make sure to define self.net as a neural network module in your __init__ method."
            )
        return self.net(X)

    def loss(self, y_hat, y) -> torch.Tensor:
        """Â≠êÁ±ªÂøÖÈ°ªÂÆûÁé∞Ëøô‰∏™ÊñπÊ≥ïÊù•ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞„ÄÇ"""
        raise NotImplementedError

    def training_step(self, batch) -> torch.Tensor:
        """
        Defines the logic for one training step.
        Just computes and returns the loss. No plotting!
        """
        # *batch[:-1] Â∞Ü batch ‰∏≠Èô§ÊúÄÂêé‰∏Ä‰∏™ÂÖÉÁ¥†Â§ñÁöÑÊâÄÊúâÂÖÉÁ¥†‰Ωú‰∏∫ËæìÂÖ•
        # batch[-1] ÊòØÊ†áÁ≠æ y
        X, y = batch[:-1], batch[-1]
        y_hat = self(*X)  # Ë∞ÉÁî® self.forward(X)
        return self.loss(y_hat, y)

    def validation_step(self, batch) -> torch.Tensor:
        """

        Defines the logic for one validation step.
        Just computes and returns the loss.
        """
        X, y = batch[:-1], batch[-1]
        y_hat = self(*X)
        return self.loss(y_hat, y)

    def configure_optimizers(self) -> torch.optim.Optimizer:
        """Â≠êÁ±ªÂøÖÈ°ªÂÆûÁé∞Ëøô‰∏™ÊñπÊ≥ïÊù•ËøîÂõû‰ºòÂåñÂô®„ÄÇ"""
        # ËøôÊòØ‰∏Ä‰∏™Â∏∏ËßÅÁöÑÂÆûÁé∞ÔºåÂèØ‰ª•ÊîæÂú®Âü∫Á±ª‰∏≠
        return torch.optim.SGD(self.parameters(), lr=self.config.lr)

    
# ÂºïÂÖ•Êàë‰ª¨‰πãÂâçÂÆûÁé∞ÁöÑ Pydantic Áâà ProgressBoard
# from your_file import ProgressBoard, BoardConfig


class Callback:
    """Base class for callbacks."""

    def __init__(self):
        """Initialize the callback."""
        self.trainer: Optional["Trainer"] = None

    def on_train_begin(self, trainer: "Trainer"):
        pass

    def on_train_end(self, trainer: "Trainer"):
        pass

    def on_epoch_begin(self, trainer: "Trainer"):
        pass

    def on_epoch_end(self, trainer: "Trainer", train_metrics: dict, val_metrics: dict):
        pass

    def on_batch_begin(self, trainer: "Trainer"):
        pass

    def on_batch_end(self, trainer: "Trainer", loss: torch.Tensor):
        pass


class PlottingCallback(Callback):
    """A callback dedicated to plotting metrics during training."""

    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):
        # ÂàõÂª∫ÁªòÂõæÊùø
        matplotlib.use("inline")  # Á°Æ‰øùÂú® Jupyter Notebook ‰∏≠‰ΩøÁî®ÂÜÖËÅîÁªòÂõæ
        self.board = ProgressBoard()  # ‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ
        self.plot_train_per_epoch = plot_train_per_epoch
        self.plot_valid_per_epoch = plot_valid_per_epoch

    def on_train_begin(self, trainer: "Trainer"):
        # Âú®ËÆ≠ÁªÉÂºÄÂßãÊó∂ËÆæÁΩÆxËΩ¥Ê†áÁ≠æ
        self.board.config.xlabel = "epoch"

    def on_batch_end(self, trainer, loss):
        """Âú®ÊØè‰∏™ËÆ≠ÁªÉÊâπÊ¨°ÁªìÊùüÂêéÔºåÁªòÂà∂ËÆ≠ÁªÉÊçüÂ§±„ÄÇ"""
        # ËÆ°ÁÆóÂΩìÂâçÂú® epoch ‰∏≠ÁöÑ‰ΩçÁΩÆ (0.0 to 1.0)
        x = trainer.epoch + (trainer.train_batch_idx + 1) / trainer.num_train_batches

        # ËÆ°ÁÆóÁªòÂõæÈ¢ëÁéá
        n = trainer.num_train_batches / self.plot_train_per_epoch

        self.board.draw(x, loss.item(), "train_loss", every_n=int(n))

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        """Âú®ÊØè‰∏™ËΩÆÊ¨°ÁªìÊùüÂêéÔºåÁªòÂà∂ËÅöÂêàÂêéÁöÑÈ™åËØÅÊçüÂ§±„ÄÇ"""
        # xËΩ¥ÊòØÂΩìÂâçepochÊï∞ (e.g., 1, 2, 3...)
        x = trainer.epoch + 1
        self.board.draw(x, val_metrics["loss"], "val_loss", every_n=1)

class LiveLossCallback(Callback):
    """‰ΩøÁî® livelossplot ÁöÑÂõûË∞É"""
    
    def __init__(self):
        super().__init__()
        self.liveloss = PlotLosses()
    
    def on_epoch_end(self, trainer, train_metrics: dict, val_metrics: dict):
        """Âú®ÊØè‰∏™ epoch ÁªìÊùüÊó∂Êõ¥Êñ∞ÂõæË°®"""
        logs = {}
        
        # Ê∑ªÂä†ËÆ≠ÁªÉÊåáÊ†á
        if train_metrics:
            logs['loss'] = train_metrics['loss']
            if 'accuracy' in train_metrics:
                logs['accuracy'] = train_metrics['accuracy']
        
        # Ê∑ªÂä†È™åËØÅÊåáÊ†áÔºàËá™Âä®Âä† val_ ÂâçÁºÄÔºâ
        if val_metrics:
            logs['val_loss'] = val_metrics['loss']
            if 'accuracy' in val_metrics:
                logs['val_accuracy'] = val_metrics['accuracy']
        
        # Êõ¥Êñ∞Âπ∂ÊòæÁ§∫ÂõæË°®
        self.liveloss.update(logs)
        self.liveloss.send()



class SystemInfoCallback(Callback):
    """ÊòæÁ§∫Á≥ªÁªüÂíåÁ°¨‰ª∂‰ø°ÊÅØÁöÑÂõûË∞É"""
    
    def __init__(self, show_detailed=False):
        super().__init__()
        self.show_detailed = show_detailed
    
    def on_train_begin(self, trainer: "Trainer"):
        print("üñ•Ô∏è  Á≥ªÁªü‰ø°ÊÅØ:")
        print(f"   Êìç‰ΩúÁ≥ªÁªü: {platform.system()} {platform.release()}")
        print(f"   Python ÁâàÊú¨: {platform.python_version()}")
        print(f"   PyTorch ÁâàÊú¨: {torch.__version__}")
        
        # ÊòæÁ§∫Ê®°ÂûãÂΩìÂâç‰ΩøÁî®ÁöÑËÆæÂ§á
        if hasattr(trainer.model, 'device'):
            print(f"üéØ Ê®°ÂûãËÆæÂ§á: {trainer.model.device}")
        
        # GPU ‰ø°ÊÅØ
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(current_device)
            memory_allocated = torch.cuda.memory_allocated(current_device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(current_device) / 1024**3
            
            print(f"üöÄ GPU ‰ø°ÊÅØ:")
            print(f"   ËÆæÂ§áÊï∞Èáè: {device_count}")
            print(f"   ÂΩìÂâçËÆæÂ§á: {current_device} ({device_name})")
            print(f"   Â∑≤ÂàÜÈÖçÂÜÖÂ≠ò: {memory_allocated:.2f} GB")
            print(f"   Â∑≤‰øùÁïôÂÜÖÂ≠ò: {memory_reserved:.2f} GB")
            
            if self.show_detailed:
                for i in range(device_count):
                    print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("‚ö†Ô∏è  Êú™Ê£ÄÊµãÂà∞ÂèØÁî®ÁöÑ GPUÔºå‰ΩøÁî® CPU")

        # CPU ‰ø°ÊÅØ
        cpu_count = psutil.cpu_count(logical=False)
        cpu_count_logical = psutil.cpu_count(logical=True)
        memory = psutil.virtual_memory()
        
        print(f"üíª CPU ‰ø°ÊÅØ:")
        print(f"   Áâ©ÁêÜÊ†∏ÂøÉÊï∞: {cpu_count}")
        print(f"   ÈÄªËæëÊ†∏ÂøÉÊï∞: {cpu_count_logical}")
        print(f"   ÊÄªÂÜÖÂ≠ò: {memory.total / 1024**3:.1f} GB")
        print(f"   ÂèØÁî®ÂÜÖÂ≠ò: {memory.available / 1024**3:.1f} GB")
        
        if self.show_detailed:
            print(f"   CPU ‰ΩøÁî®Áéá: {psutil.cpu_percent(interval=1):.1f}%")
            print(f"   ÂÜÖÂ≠ò‰ΩøÁî®Áéá: {memory.percent:.1f}%")
        
        print()  # Á©∫Ë°åÂàÜÈöî


class TrainingProgressCallback(Callback):
    """ÊòæÁ§∫ËÆ≠ÁªÉËøõÂ∫¶‰ø°ÊÅØÁöÑÂõûË∞É"""
    
    def __init__(self, show_time_estimate=True):
        super().__init__()
        self.show_time_estimate = show_time_estimate
        self.start_time:float = 0.0
        self.epoch_start_time:float = 0.0  
    
    def on_train_begin(self, trainer: "Trainer"):
        self.start_time = time.time()
        
        print("üìä ËÆ≠ÁªÉÈÖçÁΩÆ:")
        print(f"   ÊÄªËΩÆÊ¨°: {trainer.max_epochs}")
        print(f"   ËÆ≠ÁªÉÊâπÊ¨°Êï∞: {trainer.num_train_batches}")
        if trainer.val_loader:
            print(f"   È™åËØÅÊâπÊ¨°Êï∞: {trainer.num_val_batches}")
        
        # ÊòæÁ§∫Ê®°Âûã‰ø°ÊÅØ - Â§ÑÁêÜÊáíÂä†ËΩΩÊ®°Âùó
        try:
            total_params = sum(p.numel() for p in trainer.model.parameters())
            trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
            print(f"   Ê®°ÂûãÂèÇÊï∞ÊÄªÊï∞: {total_params:,}")
            print(f"   ÂèØËÆ≠ÁªÉÂèÇÊï∞: {trainable_params:,}")
        except ValueError as e:
            if "uninitialized parameter" in str(e):
                print("   Ê®°ÂûãÂèÇÊï∞: ‰ΩøÁî®ÊáíÂä†ËΩΩÊ®°ÂùóÔºåÂ∞ÜÂú®Á¨¨‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠ÂêéÊòæÁ§∫")
            else:
                raise e
        
        print(f"   ‰ºòÂåñÂô®: {type(trainer.model.configure_optimizers()).__name__}")
        print()
    
    def on_epoch_begin(self, trainer: "Trainer"):
        self.epoch_start_time = time.time()
        print(f"üîÑ Epoch {trainer.epoch + 1}/{trainer.max_epochs}")
    
    def on_epoch_end(self, trainer: "Trainer", train_metrics: dict, val_metrics: dict):
        import time
        if self.epoch_start_time:
            epoch_time = time.time() - self.epoch_start_time
            print(f"   ËÆ≠ÁªÉÊçüÂ§±: {train_metrics['loss']:.4f}")
            if val_metrics:
                print(f"   È™åËØÅÊçüÂ§±: {val_metrics['loss']:.4f}")
            print(f"   Áî®Êó∂: {epoch_time:.2f}s")
            
            # ‰º∞ÁÆóÂâ©‰ΩôÊó∂Èó¥
            if self.show_time_estimate and trainer.epoch > 0:
                elapsed_time = time.time() - self.start_time
                avg_time_per_epoch = elapsed_time / (trainer.epoch + 1)
                remaining_epochs = trainer.max_epochs - trainer.epoch - 1
                eta = remaining_epochs * avg_time_per_epoch
                
                if eta > 60:
                    print(f"   È¢ÑËÆ°Ââ©‰ΩôÊó∂Èó¥: {eta/60:.1f} ÂàÜÈíü")
                else:
                    print(f"   È¢ÑËÆ°Ââ©‰ΩôÊó∂Èó¥: {eta:.0f} Áßí")
            print()
    
    def on_train_end(self, trainer: "Trainer"):
        import time
        if self.start_time:
            total_time = time.time() - self.start_time
            print(f"‚úÖ ËÆ≠ÁªÉÂÆåÊàêÔºÅÊÄªÁî®Êó∂: {total_time/60:.1f} ÂàÜÈíü")


class ModelSummaryCallback(Callback):
    """ÊòæÁ§∫Ê®°ÂûãÊû∂ÊûÑÊëòË¶ÅÁöÑÂõûË∞É"""
    
    def on_train_begin(self, trainer: "Trainer"):
        print("üèóÔ∏è  Ê®°ÂûãÊû∂ÊûÑ:")
        print(trainer.model)
        print()


class MemoryMonitorCallback(Callback):
    """ÁõëÊéßÂÜÖÂ≠ò‰ΩøÁî®ÁöÑÂõûË∞É"""
    
    def __init__(self, check_every_n_epochs=1):
        super().__init__()
        self.check_every_n_epochs = check_every_n_epochs
    
    def on_epoch_end(self, trainer: "Trainer", train_metrics: dict, val_metrics: dict):
        if (trainer.epoch + 1) % self.check_every_n_epochs == 0:
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"   GPU ÂÜÖÂ≠ò - Â∑≤ÂàÜÈÖç: {memory_allocated:.2f}GB, Â∑≤‰øùÁïô: {memory_reserved:.2f}GB")
            
            memory = psutil.virtual_memory()
            print(f"   Á≥ªÁªüÂÜÖÂ≠ò‰ΩøÁî®Áéá: {memory.percent:.1f}%")


# ÂàõÂª∫‰∏Ä‰∏™‰æøÊç∑ÁöÑÁªÑÂêàÂõûË∞É
class DefaultCallbacks:
    """Êèê‰æõÂ∏∏Áî®ÂõûË∞ÉÁªÑÂêàÁöÑÂ∑•ÂéÇÁ±ª"""
    
    @staticmethod
    def basic():
        """Âü∫Á°ÄÂõûË∞ÉÁªÑÂêà"""
        return [
            SystemInfoCallback(),
            TrainingProgressCallback(),
        ]
    
    @staticmethod
    def detailed():
        """ËØ¶ÁªÜÂõûË∞ÉÁªÑÂêà"""
        return [
            SystemInfoCallback(show_detailed=True),
            ModelSummaryCallback(),
            TrainingProgressCallback(),
            MemoryMonitorCallback(),
        ]
    
    @staticmethod
    def with_live_loss():
        """ÂåÖÂê´ÂÆûÊó∂ÊçüÂ§±ÂõæÁöÑÂõûË∞ÉÁªÑÂêà"""
        return [
            SystemInfoCallback(),
            TrainingProgressCallback(),
            LiveLossCallback(),
        ]


class Trainer:
    """The conductor that orchestrates the training process."""

    _model: Optional[ModernModule] = None
    _train_loader: Optional[torch.utils.data.DataLoader] = None
    _val_loader: Optional[torch.utils.data.DataLoader] = None
    def __init__(self, max_epochs: int, callbacks: List[Callback] = []):
        self.max_epochs = max_epochs
        self.callbacks = callbacks or []

        # ÊòéÁ°ÆÂ£∞ÊòéÂ∞ÜÂú® fit ÊñπÊ≥ï‰∏≠ËÆæÁΩÆÁöÑÂ±ûÊÄß
        self.epoch: int = 0
        self.train_batch_idx: int = 0
        self.val_batch_idx: int = 0
        self.num_train_batches: int = 0
        self.num_val_batches: int = 0

    @property
    def model(self) -> ModernModule:
        assert self._model is not None, "Model must be set before accessing."
        return self._model
    @model.setter
    def model(self, value: ModernModule):
        if not isinstance(value, ModernModule):
            raise TypeError("Model must be an instance of ModernModule.")
        self._model = value
    @property
    def train_loader(self) -> torch.utils.data.DataLoader:
        assert self._train_loader is not None, "Train loader must be set before accessing."
        return self._train_loader
    @train_loader.setter
    def train_loader(self, value: torch.utils.data.DataLoader):
        self._train_loader = value
        
    @property
    def val_loader(self) -> Optional[torch.utils.data.DataLoader]:
        assert self._val_loader is not None, "Validation loader must be set before accessing."
        return self._val_loader
    @val_loader.setter
    def val_loader(self, value: Optional[torch.utils.data.DataLoader]):
        self._val_loader = value

    def fit(self, model: ModernModule, train_loader, val_loader=None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader

        # Â∞Ü self (trainer) Ê≥®ÂÖ•Âà∞ÂõûË∞É‰∏≠ÔºåËÆ©ÂõûË∞ÉÂèØ‰ª•ËÆøÈóÆËÆ≠ÁªÉÁä∂ÊÄÅ
        for cb in self.callbacks:
            cb.trainer = self

        optimizer = self.model.configure_optimizers()

        self.num_train_batches = len(self.train_loader) if self.train_loader else 0
        self.num_val_batches = len(self.val_loader) if self.val_loader else 0

        # --- ËÆ≠ÁªÉÂæ™ÁéØ ---
        self._dispatch("on_train_begin")
        for self.epoch in range(self.max_epochs):
            self._dispatch("on_epoch_begin")

            # Training loop for one epoch

            self.model.train()
            total_train_loss = 0.0
            for self.train_batch_idx, batch in enumerate(self.train_loader):
                self._dispatch("on_batch_begin")
                optimizer.zero_grad()
                loss = self.model.training_step(batch)
                loss.backward()
                optimizer.step()
                self._dispatch("on_batch_end", loss)

                # Á°Æ‰øù loss.item() ËøîÂõû float Á±ªÂûã
                loss_value = loss.item()
                if isinstance(loss_value, bool):
                    raise TypeError(
                        f"Loss function returned bool value: {loss_value}. Check your loss function implementation."
                    )
                total_train_loss += float(loss_value)

            # ÈÅøÂÖçÈô§Èõ∂ÈîôËØØ
            train_metrics = {"loss": total_train_loss / max(self.num_train_batches, 1)}

            # Validation loop for one epoch
            val_metrics = {}
            if self.val_loader and self.num_val_batches > 0:
                self.model.eval()
                total_val_loss = 0.0
                with torch.no_grad():
                    for self.val_batch_idx, batch in enumerate(self.val_loader):
                        loss = self.model.validation_step(batch)

                        # Á°Æ‰øù loss.item() ËøîÂõû float Á±ªÂûã
                        loss_value = loss.item()
                        if isinstance(loss_value, bool):
                            raise TypeError(
                                f"Validation loss function returned bool value: {loss_value}. Check your loss function implementation."
                            )
                        total_val_loss += float(loss_value)

                val_metrics = {"loss": total_val_loss / self.num_val_batches}

            self._dispatch("on_epoch_end", train_metrics, val_metrics)

        self._dispatch("on_train_end")

    def _dispatch(self, event_name: str, *args):
        """Calls the corresponding method on all callbacks."""
        for cb in self.callbacks:
            if hasattr(cb, event_name):
                getattr(cb, event_name)(self, *args)
