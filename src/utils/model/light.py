from pydantic import BaseModel
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback, ModelCheckpoint, EarlyStopping
from typing import List, Optional, Union
import matplotlib.pyplot as plt
from livelossplot import PlotLosses

from torch import nn
import numpy as np





# æ·»åŠ è‡ªå®šä¹‰å›è°ƒæ¥é›†æˆ livelossplot
class LiveLossPlotCallback(pl.Callback):
    def __init__(self):
        super().__init__()
        self.liveloss = PlotLosses()
    
    def on_validation_epoch_end(self, trainer, pl_module):
        logs = {}
        
        # è·å–è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
        if 'train_loss' in trainer.callback_metrics:
            logs['log loss'] = trainer.callback_metrics['train_loss'].item()
        if 'val_loss' in trainer.callback_metrics:
            logs['val_log loss'] = trainer.callback_metrics['val_loss'].item()
        if 'train_acc' in trainer.callback_metrics:
            logs['accuracy'] = trainer.callback_metrics['train_acc'].item()
        if 'val_acc' in trainer.callback_metrics:
            logs['val_accuracy'] = trainer.callback_metrics['val_acc'].item()
        
        # æ›´æ–°å›¾è¡¨
        if logs:
            self.liveloss.update(logs)
            self.liveloss.send()


class MutualInformationCallback(Callback):
    def __init__(self, mi_model, eval_loader, eval_every_n_epochs=1):
        super().__init__()  # æ·»åŠ è¿™è¡Œ
        self.mi_model = mi_model
        self.eval_loader = eval_loader
        self.eval_every_n_epochs = eval_every_n_epochs
        self.mi_history = {'epoch': [], 'layer_results': []}
        print(f"âœ… MutualInformationCallback åˆå§‹åŒ–æˆåŠŸï¼Œæ¯ {eval_every_n_epochs} ä¸ªepochåˆ†æä¸€æ¬¡")
    
    def on_train_epoch_end(self, trainer, pl_module):
        """æ”¹ç”¨on_train_epoch_endè€Œä¸æ˜¯on_epoch_end"""
        current_epoch = trainer.current_epoch
        
        print(f"\nğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦MIåˆ†æ - Epoch {current_epoch}")
        
        if current_epoch % self.eval_every_n_epochs == 0:
            print(f"=== Epoch {current_epoch} äº’ä¿¡æ¯åˆ†æå¼€å§‹ ===")
            
            try:
                # åˆ›å»ºMIåˆ†æå™¨
                mi_analyzer = self._create_mi_analyzer(pl_module)
                
                # æ”¶é›†æ•°æ®ï¼ˆåªç”¨å°‘é‡æ•°æ®é¿å…å†…å­˜é—®é¢˜ï¼‰
                print("ğŸ“Š æ”¶é›†è¯„ä¼°æ•°æ®...")
                X_list, y_list = [], []
                for i, (X, y) in enumerate(self.eval_loader):
                    X_list.append(X)
                    y_list.append(y)
                    if i >= 2:  # åªç”¨3ä¸ªbatch
                        break
                
                if not X_list:
                    print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
                    return
                
                X_combined = torch.cat(X_list, dim=0)
                y_combined = torch.cat(y_list, dim=0)
                print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå½¢çŠ¶: {X_combined.shape}")
                
                # åˆ†æå„å±‚
                layer_results = {}
                layers = ['conv1', 'conv2', 'fc1', 'fc2', 'fc3']
                
                for layer_name in layers:
                    print(f"ğŸ”¬ åˆ†æå±‚: {layer_name}")
                    try:
                        results = mi_analyzer.analyze_layer(X_combined, y_combined, layer_name)
                        if results:
                            layer_results[layer_name] = results
                            print(f"  âœ… {layer_name}: I(X;T)={results.get('I(X;T)', 0):.4f}, "
                                  f"I(T;Y)={results.get('I(T;Y)', 0):.4f}")
                        else:
                            print(f"  âŒ {layer_name}: åˆ†æå¤±è´¥")
                    except Exception as e:
                        print(f"  âŒ {layer_name}: åˆ†æå‡ºé”™ - {e}")
                
                # è®°å½•å†å²
                self.mi_history['epoch'].append(current_epoch)
                self.mi_history['layer_results'].append(layer_results)
                
                print(f"=== Epoch {current_epoch} äº’ä¿¡æ¯åˆ†æå®Œæˆ ===\n")
                
            except Exception as e:
                print(f"âŒ äº’ä¿¡æ¯åˆ†æå‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
    
    def _create_mi_analyzer(self, trained_model):
        """åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹åˆ›å»ºMIåˆ†æå™¨"""
        class MIAnalyzer(nn.Module):
            def __init__(self, source_model):
                super().__init__()
                self.net = source_model.net
                self.activations = {}
                self._register_hooks()
            
            def _register_hooks(self):
                def hook_fn(name):
                    def hook(module, input, output):
                        self.activations[name] = output.detach()
                    return hook
                
                # ä¸ºå…³é”®å±‚æ³¨å†Œé’©å­
                try:
                    self.net[0].register_forward_hook(hook_fn('conv1'))  # ç¬¬ä¸€ä¸ªå·ç§¯
                    self.net[3].register_forward_hook(hook_fn('conv2'))  # ç¬¬äºŒä¸ªå·ç§¯
                    self.net[7].register_forward_hook(hook_fn('fc1'))    # ç¬¬ä¸€ä¸ªå…¨è¿æ¥
                    self.net[9].register_forward_hook(hook_fn('fc2'))    # ç¬¬äºŒä¸ªå…¨è¿æ¥
                    self.net[11].register_forward_hook(hook_fn('fc3'))   # è¾“å‡ºå±‚
                    print("âœ… Hookæ³¨å†ŒæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ Hookæ³¨å†Œå¤±è´¥: {e}")
            
            def forward(self, x):
                return self.net(x)
                
            def analyze_layer(self, X, y, target_layer, sample_size=100):
                """åˆ†æç‰¹å®šå±‚çš„äº’ä¿¡æ¯"""
                self.eval()
                
                # é‡‡æ ·æ•°æ®
                if len(X) > sample_size:
                    indices = torch.randperm(len(X))[:sample_size]
                    X_sample = X[indices]
                    y_sample = y[indices]
                else:
                    X_sample = X
                    y_sample = y
                
                # å‰å‘ä¼ æ’­
                with torch.no_grad():
                    _ = self.forward(X_sample)
                
                if target_layer not in self.activations:
                    print(f"âŒ å±‚ {target_layer} çš„æ¿€æ´»æœªæ‰¾åˆ°")
                    return {}
                
                T = self.activations[target_layer]
                print(f"ğŸ“Š {target_layer} æ¿€æ´»å½¢çŠ¶: {T.shape}")
                
                try:
                    # ç®€åŒ–ç‰ˆæœ¬çš„äº’ä¿¡æ¯ä¼°è®¡
                    results = {}
                    
                    # è®¡ç®—I(X;T) - ç®€åŒ–ç‰ˆæœ¬
                    X_flat = X_sample.view(X_sample.size(0), -1)
                    T_flat = T.view(T.size(0), -1)
                    
                    # ä½¿ç”¨ç›¸å…³æ€§ä½œä¸ºäº’ä¿¡æ¯çš„ä»£ç†
                    if X_flat.size(1) > 0 and T_flat.size(1) > 0:
                        # è®¡ç®—ç¬¬ä¸€ä¸ªç‰¹å¾çš„ç›¸å…³æ€§
                        x_feat = X_flat[:, 0].cpu().numpy()
                        t_feat = T_flat[:, 0].cpu().numpy()
                        
                        corr_xt = np.corrcoef(x_feat, t_feat)[0, 1]
                        if not np.isnan(corr_xt):
                            I_XT = -0.5 * np.log(1 - corr_xt**2 + 1e-8)
                            results['I(X;T)'] = I_XT
                    
                    # è®¡ç®—I(T;Y) - ç®€åŒ–ç‰ˆæœ¬
                    if T_flat.size(1) > 0:
                        y_np = y_sample.cpu().numpy()
                        t_feat = T_flat[:, 0].cpu().numpy()
                        
                        # å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡å€¼å·®å¼‚
                        unique_classes = np.unique(y_np)
                        if len(unique_classes) > 1:
                            class_means = []
                            for cls in unique_classes:
                                mask = y_np == cls
                                if np.sum(mask) > 0:
                                    class_means.append(np.mean(t_feat[mask]))
                            
                            if len(class_means) > 1:
                                var_between = np.var(class_means)
                                var_within = np.var(t_feat)
                                if var_within > 0:
                                    I_TY = 0.5 * np.log(1 + var_between / var_within)
                                    results['I(T;Y)'] = I_TY
                    
                    return results
                    
                except Exception as e:
                    print(f"âŒ äº’ä¿¡æ¯è®¡ç®—å‡ºé”™: {e}")
                    return {}
        
        return MIAnalyzer(trained_model)

# æ·»åŠ å¯è§†åŒ–å›è°ƒç±»
class PlotMetricsCallback(Callback):
    """å®æ—¶ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡çš„å›è°ƒå‡½æ•°"""
    
    def __init__(self, plot_every_n_epochs=1):
        super().__init__()
        self.plot_every_n_epochs = plot_every_n_epochs
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.epochs = []
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def on_validation_epoch_end(self, trainer, pl_module):
        """åœ¨æ¯ä¸ªéªŒè¯epochç»“æŸæ—¶æ›´æ–°å›¾è¡¨"""
        current_epoch = trainer.current_epoch
        
        # æ”¶é›†æŒ‡æ ‡
        train_loss = trainer.callback_metrics.get('train_loss', None)
        val_loss = trainer.callback_metrics.get('val_loss', None)
        train_acc = trainer.callback_metrics.get('train_acc', None)
        val_acc = trainer.callback_metrics.get('val_acc', None)
        
        if train_loss is not None and val_loss is not None:
            self.epochs.append(current_epoch)
            self.train_losses.append(train_loss.item())
            self.val_losses.append(val_loss.item())
            
            if train_acc is not None and val_acc is not None:
                self.train_accs.append(train_acc.item())
                self.val_accs.append(val_acc.item())
        
        # æ¯éš”æŒ‡å®šè½®æ•°ç»˜åˆ¶ä¸€æ¬¡å›¾è¡¨
        if (current_epoch + 1) % self.plot_every_n_epochs == 0 and len(self.epochs) > 0:
            self.plot_metrics()
    
    def plot_metrics(self):
        """ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # ç»˜åˆ¶æŸå¤±
        axes[0].plot(self.epochs, self.train_losses, 'b-', label='è®­ç»ƒæŸå¤±', marker='o', markersize=4)
        axes[0].plot(self.epochs, self.val_losses, 'r-', label='éªŒè¯æŸå¤±', marker='s', markersize=4)
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('æŸå¤±å‡½æ•°å˜åŒ–')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # ç»˜åˆ¶å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if len(self.train_accs) > 0:
            axes[1].plot(self.epochs, self.train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', marker='o', markersize=4)
            axes[1].plot(self.epochs, self.val_accs, 'r-', label='éªŒè¯å‡†ç¡®ç‡', marker='s', markersize=4)
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].set_title('å‡†ç¡®ç‡å˜åŒ–')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            axes[1].set_ylim(0, 1)
        else:
            axes[1].text(0.5, 0.5, 'æš‚æ— å‡†ç¡®ç‡æ•°æ®', ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('å‡†ç¡®ç‡å˜åŒ–')
        
        plt.tight_layout()
        plt.show()
        
        # æ¸…é™¤è¾“å‡ºï¼Œé˜²æ­¢å›¾è¡¨å †ç§¯
        from IPython.display import clear_output
        clear_output(wait=True)
        plt.show()

# æ·»åŠ æ›´é¢‘ç¹çš„å®æ—¶ç»˜å›¾å›è°ƒ
class HighFrequencyLiveLossPlotCallback(pl.Callback):
    """é«˜é¢‘ç‡çš„ livelossplot å›è°ƒ - æ¯éš” N ä¸ª batch æ›´æ–°ä¸€æ¬¡"""
    
    def __init__(self, update_every_n_batches=10):
        super().__init__()
        self.liveloss = PlotLosses()
        self.update_every_n_batches = update_every_n_batches
        self.batch_count = 0
        
        # ç”¨äºç´¯ç§¯æŒ‡æ ‡
        self.train_loss_accumulator = []
        self.current_epoch = 0
        
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """æ¯ä¸ªè®­ç»ƒbatchç»“æŸåè°ƒç”¨"""
        self.batch_count += 1
        
        # ç´¯ç§¯å½“å‰batchçš„æŸå¤±
        if outputs is not None:
            if isinstance(outputs, dict) and 'loss' in outputs:
                self.train_loss_accumulator.append(outputs['loss'].item())
            elif hasattr(outputs, 'item'):  # å¤„ç†ç›´æ¥è¿”å›tensorçš„æƒ…å†µ
                self.train_loss_accumulator.append(outputs.item())
            elif torch.is_tensor(outputs):  # å¤„ç†tensoræƒ…å†µ
                self.train_loss_accumulator.append(outputs.item())
        
        # æ¯éš” N ä¸ª batch æ›´æ–°ä¸€æ¬¡å›¾è¡¨
        if self.batch_count % self.update_every_n_batches == 0:
            self._update_plot(trainer, pl_module)
    
    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯ç»“æŸåä¹Ÿæ›´æ–°ä¸€æ¬¡"""
        self._update_plot(trainer, pl_module)
        self.current_epoch += 1
        self.train_loss_accumulator = []  # é‡ç½®ç´¯ç§¯å™¨
    
    def _update_plot(self, trainer, pl_module):
        """æ›´æ–°å›¾è¡¨"""
        logs = {}
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        if self.train_loss_accumulator:
            avg_train_loss = sum(self.train_loss_accumulator) / len(self.train_loss_accumulator)
            logs['log loss'] = avg_train_loss
        
        # è·å–æœ€æ–°çš„éªŒè¯æŒ‡æ ‡
        if 'val_loss' in trainer.callback_metrics:
            logs['val_log loss'] = trainer.callback_metrics['val_loss'].item()
        if 'val_acc' in trainer.callback_metrics:
            logs['val_accuracy'] = trainer.callback_metrics['val_acc'].item()
        if 'train_acc' in trainer.callback_metrics:
            logs['accuracy'] = trainer.callback_metrics['train_acc'].item()
        
        # æ›´æ–°å›¾è¡¨
        if logs:
            self.liveloss.update(logs)
            self.liveloss.send()

class RealTimePlotCallback(pl.Callback):
    """çœŸæ­£çš„å®æ—¶ç»˜å›¾å›è°ƒ - ä½¿ç”¨matplotlibçš„åŠ¨æ€æ›´æ–°"""
    
    def __init__(self, update_every_n_batches=5):
        super().__init__()
        self.update_every_n_batches = update_every_n_batches
        self.batch_count = 0
        
        # æ•°æ®å­˜å‚¨
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.batch_indices = []
        
        # è®¾ç½®matplotlibä¸ºäº¤äº’æ¨¡å¼
        plt.ion()
        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(12, 4))
        self.fig.suptitle('å®æ—¶è®­ç»ƒç›‘æ§')
        
        # åˆå§‹åŒ–ç©ºçº¿æ¡
        self.train_loss_line, = self.ax1.plot([], [], 'b-', label='è®­ç»ƒæŸå¤±', alpha=0.7)
        self.val_loss_line, = self.ax1.plot([], [], 'r-', label='éªŒè¯æŸå¤±', marker='o', markersize=3)
        
        self.train_acc_line, = self.ax2.plot([], [], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.7)
        self.val_acc_line, = self.ax2.plot([], [], 'r-', label='éªŒè¯å‡†ç¡®ç‡', marker='o', markersize=3)
        
        # è®¾ç½®å›¾è¡¨
        self._setup_plots()
        
    def _setup_plots(self):
        """è®¾ç½®å›¾è¡¨æ ·å¼"""
        # æŸå¤±å›¾
        self.ax1.set_xlabel('Batch')
        self.ax1.set_ylabel('Loss')
        self.ax1.set_title('æŸå¤±å˜åŒ–')
        self.ax1.legend()
        self.ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡å›¾
        self.ax2.set_xlabel('Batch')
        self.ax2.set_ylabel('Accuracy')
        self.ax2.set_title('å‡†ç¡®ç‡å˜åŒ–')
        self.ax2.legend()
        self.ax2.grid(True, alpha=0.3)
        self.ax2.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.show(block=False)
    
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """æ¯ä¸ªè®­ç»ƒbatchç»“æŸåè°ƒç”¨"""
        self.batch_count += 1
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        if outputs and 'loss' in outputs:
            self.train_losses.append(outputs['loss'].item())
            self.batch_indices.append(self.batch_count)
            
            # å¦‚æœæœ‰è®­ç»ƒå‡†ç¡®ç‡
            if 'train_acc' in trainer.callback_metrics:
                self.train_accs.append(trainer.callback_metrics['train_acc'].item())
        
        # æ¯éš” N ä¸ª batch æ›´æ–°å›¾è¡¨
        if self.batch_count % self.update_every_n_batches == 0:
            self._update_real_time_plot()
    
    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯ç»“æŸåæ›´æ–°éªŒè¯æŒ‡æ ‡"""
        if 'val_loss' in trainer.callback_metrics and self.batch_indices:
            # åœ¨å½“å‰ä½ç½®æ·»åŠ éªŒè¯ç‚¹
            current_batch = self.batch_indices[-1]
            self.val_losses.append((current_batch, trainer.callback_metrics['val_loss'].item()))
            
            if 'val_acc' in trainer.callback_metrics:
                self.val_accs.append((current_batch, trainer.callback_metrics['val_acc'].item()))
        
        self._update_real_time_plot()
    
    def _update_real_time_plot(self):
        """æ›´æ–°å®æ—¶å›¾è¡¨"""
        if not self.batch_indices:
            return
            
        # æ›´æ–°è®­ç»ƒæŸå¤±
        if self.train_losses:
            self.train_loss_line.set_data(self.batch_indices, self.train_losses)
            
        # æ›´æ–°éªŒè¯æŸå¤±
        if self.val_losses:
            val_x, val_y = zip(*self.val_losses)
            self.val_loss_line.set_data(val_x, val_y)
        
        # æ›´æ–°è®­ç»ƒå‡†ç¡®ç‡
        if self.train_accs:
            self.train_acc_line.set_data(self.batch_indices, self.train_accs)
            
        # æ›´æ–°éªŒè¯å‡†ç¡®ç‡
        if self.val_accs:
            val_acc_x, val_acc_y = zip(*self.val_accs)
            self.val_acc_line.set_data(val_acc_x, val_acc_y)
        
        # è‡ªåŠ¨è°ƒæ•´åæ ‡è½´
        for ax in [self.ax1, self.ax2]:
            ax.relim()
            ax.autoscale_view()
        
        # åˆ·æ–°å›¾è¡¨
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()
        
        # åœ¨Jupyterä¸­æ˜¾ç¤º
        from IPython.display import display, clear_output
        clear_output(wait=True)
        display(self.fig)

# æ·»åŠ å›è°ƒç»„åˆç±»
class DefaultCallbacks:
    """é»˜è®¤å›è°ƒç»„åˆç±»"""
    
    @staticmethod
    def basic() -> List[Callback]:
        """åŸºç¡€å›è°ƒ"""
        return []
    
    @staticmethod
    def with_live_plot(plot_every_n_epochs=1) -> List[Callback]:
        """å¸¦å®æ—¶ç»˜å›¾çš„å›è°ƒ"""
        return [PlotMetricsCallback(plot_every_n_epochs=plot_every_n_epochs)]
    
    @staticmethod
    def with_early_stopping(monitor="val_loss", patience=5, mode="min") -> List[Callback]:
        """å¸¦æ—©åœçš„å›è°ƒ"""
        return [EarlyStopping(
            monitor=monitor,
            patience=patience,
            mode=mode,
            verbose=True
        )]
    
    @staticmethod
    def with_checkpoint(monitor="val_loss", mode="min", save_top_k=1) -> List[Callback]:
        """å¸¦æ¨¡å‹æ£€æŸ¥ç‚¹çš„å›è°ƒ"""
        return [ModelCheckpoint(
            monitor=monitor,
            mode=mode,
            save_top_k=save_top_k,
            verbose=True
        )]
    
    @staticmethod
    def full_featured(
        plot_every_n_epochs=1,
        monitor="val_loss", 
        patience=10, 
        mode="min", 
        save_top_k=1
    ) -> List[Callback]:
        """åŠŸèƒ½å®Œæ•´çš„å›è°ƒç»„åˆ"""
        return [
            PlotMetricsCallback(plot_every_n_epochs=plot_every_n_epochs),
            EarlyStopping(
                monitor=monitor,
                patience=patience,
                mode=mode,
                verbose=True
            ),
            ModelCheckpoint(
                monitor=monitor,
                mode=mode,
                save_top_k=save_top_k,
                verbose=True
            )
        ]
    
    @staticmethod
    def for_classification(plot_every_n_epochs=1) -> List[Callback]:
        """ä¸“ä¸ºåˆ†ç±»ä»»åŠ¡è®¾è®¡çš„å›è°ƒ"""
        return [
            PlotMetricsCallback(plot_every_n_epochs=plot_every_n_epochs),
            ModelCheckpoint(
                monitor="val_acc",
                mode="max",
                save_top_k=1,
                verbose=True,
                filename='best-{epoch:02d}-{val_acc:.2f}'
            )
        ]


class ModelConfig(BaseModel):
    """Configuration for the Model."""
    
    lr: float = 0.01
    num_hiddens: int = 256
    output_size: int = 10
    weight_decay: float = 0.0
    optimizer: str = "sgd"  # sgd, adam, adamw
    dropout_rate: float = 0.5  # Dropout rate for regularization
    
    # Lightningç›¸å…³é…ç½®
    accelerator: str = "auto"  # auto, gpu, cpu, tpu
    devices: Union[int, List[int], str] = "auto"  # auto, 1, [0,1], "0,1"
    precision: Union[int, str] = 32  # 16, 32, "bf16"
    
    momentum: float = 0.9  # SGDåŠ¨é‡
    
    class Config:
        validate_assignment = True


class ModernModule(pl.LightningModule):
    """åŸºäºPyTorch Lightningçš„ç°ä»£åŒ–æ¨¡å—"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.save_hyperparameters(config.dict())
        
        # è‡ªåŠ¨ä¿å­˜é…ç½®åˆ°æ£€æŸ¥ç‚¹
        self.automatic_optimization = True
        
    def forward(self, x):
        """é»˜è®¤å‰å‘ä¼ æ’­"""
        if not hasattr(self, "net"):
            raise NotImplementedError(
                "You must define 'self.net' in your model's __init__."
            )
        if not callable(self.net):
            raise RuntimeError(
                "self.net is not callable. Make sure it's a proper nn.Module."
            )
        return self.net(x)
    
    def compute_loss(self, y_hat, y) -> torch.Tensor:
        """å­ç±»å¿…é¡»å®ç°è¿™ä¸ªæ–¹æ³•æ¥å®šä¹‰æŸå¤±å‡½æ•°"""
        raise NotImplementedError
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤ - Lightningè‡ªåŠ¨å¤„ç†è®¾å¤‡"""
        x, y = batch
        y_hat = self(x)
        loss = self.compute_loss(y_hat, y)
        
        # è®°å½•æŒ‡æ ‡
        self.log('train_loss', loss, prog_bar=True)
        
        # å¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œè®¡ç®—å‡†ç¡®ç‡
        if hasattr(self, '_is_classification') and self._is_classification:
            acc = self._compute_accuracy(y_hat, y)
            self.log('train_acc', acc, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ - Lightningè‡ªåŠ¨å¤„ç†è®¾å¤‡"""
        x, y = batch
        y_hat = self(x)
        loss = self.compute_loss(y_hat, y)
        
        # è®°å½•æŒ‡æ ‡
        self.log('val_loss', loss, prog_bar=True)
        
        # å¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œè®¡ç®—å‡†ç¡®ç‡
        if hasattr(self, '_is_classification') and self._is_classification:
            acc = self._compute_accuracy(y_hat, y)
            self.log('val_acc', acc, prog_bar=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤"""
        x, y = batch
        y_hat = self(x)
        loss = self.compute_loss(y_hat, y)
        
        self.log('test_loss', loss)
        
        if hasattr(self, '_is_classification') and self._is_classification:
            acc = self._compute_accuracy(y_hat, y)
            self.log('test_acc', acc)
        
        return loss
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        params = self.parameters()
        
        if self.config.optimizer.lower() == "adam":
            optimizer = torch.optim.Adam(
                params, 
                lr=self.config.lr, 
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer.lower() == "adamw":
            optimizer = torch.optim.AdamW(
                params, 
                lr=self.config.lr, 
                weight_decay=self.config.weight_decay
            )
        else:  # sgd
            optimizer = torch.optim.SGD(
                params, 
                lr=self.config.lr, 
                weight_decay=self.config.weight_decay
            )
        
        # å¯é€‰ï¼šæ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        # return [optimizer], [scheduler]
        
        return optimizer
    
    def _compute_accuracy(self, y_hat, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if y_hat.dim() > 1 and y_hat.size(1) > 1:  # å¤šåˆ†ç±»
            pred = y_hat.argmax(dim=1)
        else:  # äºŒåˆ†ç±»
            pred = (y_hat > 0.5).float()
        
        return (pred == y).float().mean()
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        """é¢„æµ‹æ­¥éª¤"""
        x, _ = batch if isinstance(batch, (list, tuple)) else (batch, None)
        return self(x)

class ModernTrainer:
    """ç°ä»£åŒ–è®­ç»ƒå™¨ï¼ŒåŸºäºPyTorch Lightning"""
    
    def __init__(
        self,
        max_epochs: int = 10,
        accelerator: str = "auto",
        devices: Union[int, List[int], str] = "auto",
        precision: Union[int, str] = 32,
        log_every_n_steps: int = 50,
        enable_checkpointing: bool = True,
        enable_progress_bar: bool = True,
        logger: bool = True,
        callbacks: Optional[List[Callback]] = None,  # ä¿®æ”¹ä¸º Optional
    ):
        self.trainer_kwargs = {
            "max_epochs": max_epochs,
            "accelerator": accelerator,
            "devices": devices,
            "precision": precision,
            "log_every_n_steps": log_every_n_steps,
            "enable_checkpointing": enable_checkpointing,
            "enable_progress_bar": enable_progress_bar,
            "logger": logger,
        }

        self.callbacks: List[Callback] = callbacks or []  # å®‰å…¨çš„é»˜è®¤å€¼å¤„ç†
        self.trainer = None
    
    def add_callback(self, callback: Callback):
        """æ·»åŠ å›è°ƒ"""
        self.callbacks.append(callback)
        return self
    
    def add_callbacks(self, callbacks: List[Callback]):
        """æ‰¹é‡æ·»åŠ å›è°ƒ"""
        self.callbacks.extend(callbacks)
        return self
    
    def add_early_stopping(self, monitor="val_loss", patience=5, mode="min"):
        """æ·»åŠ æ—©åœå›è°ƒ"""
        early_stop = EarlyStopping(
            monitor=monitor,
            patience=patience,
            mode=mode,
            verbose=True
        )
        return self.add_callback(early_stop)
    
    def add_model_checkpoint(self, monitor="val_loss", mode="min", save_top_k=1):
        """æ·»åŠ æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ"""
        checkpoint = ModelCheckpoint(
            monitor=monitor,
            mode=mode,
            save_top_k=save_top_k,
            verbose=True
        )
        return self.add_callback(checkpoint)
    
    def fit(self, model: ModernModule, train_loader, val_loader=None):
        """è®­ç»ƒæ¨¡å‹"""
        # åˆ›å»ºtrainer
        self.trainer = pl.Trainer(
            callbacks=self.callbacks,
            **self.trainer_kwargs
        )
        
        # å¼€å§‹è®­ç»ƒ
        self.trainer.fit(model, train_loader, val_loader)
        
        return self.trainer
    
    def test(self, model: ModernModule, test_loader):
        """æµ‹è¯•æ¨¡å‹"""
        if self.trainer is None:
            self.trainer = pl.Trainer(**self.trainer_kwargs)
        
        return self.trainer.test(model, test_loader)
    
    def predict(self, model: ModernModule, dataloader):
        """é¢„æµ‹"""
        if self.trainer is None:
            self.trainer = pl.Trainer(**self.trainer_kwargs)
        
        return self.trainer.predict(model, dataloader)


class TrainerFactory:
    """è®­ç»ƒå™¨å·¥å‚ç±»"""
    
    @staticmethod
    def basic(max_epochs=10, callbacks=None):
        """åŸºç¡€è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            enable_progress_bar=True,
            callbacks=callbacks if callbacks is not None else DefaultCallbacks.basic()
        )
    
    @staticmethod
    def with_live_plot(max_epochs=10, plot_every_n_epochs=1):
        """å¸¦å®æ—¶ç»˜å›¾çš„è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            enable_progress_bar=True,
            callbacks=DefaultCallbacks.with_live_plot(plot_every_n_epochs)
        )
    
    @staticmethod
    def for_classification(max_epochs=10, plot_every_n_epochs=1):
        """åˆ†ç±»ä»»åŠ¡ä¸“ç”¨è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            enable_progress_bar=True,
            callbacks=DefaultCallbacks.for_classification(plot_every_n_epochs)
        )
    
    @staticmethod
    def full_featured(max_epochs=100, plot_every_n_epochs=1, patience=10):
        """åŠŸèƒ½å®Œæ•´çš„è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            enable_progress_bar=True,
            callbacks=DefaultCallbacks.full_featured(
                plot_every_n_epochs=plot_every_n_epochs,
                patience=patience
            )
        )
    
    @staticmethod
    def gpu_optimized(max_epochs=10, precision=16):
        """GPUä¼˜åŒ–è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            accelerator="gpu",
            devices=1,
            precision=precision,
            enable_progress_bar=True,
            callbacks=DefaultCallbacks.with_checkpoint()
        )
    
    @staticmethod
    def with_early_stopping(max_epochs=100, patience=10):
        """å¸¦æ—©åœçš„è®­ç»ƒå™¨"""
        return ModernTrainer(
            max_epochs=max_epochs,
            enable_progress_bar=True,
            callbacks=DefaultCallbacks.with_early_stopping(patience=patience)
        )

# è®¾å¤‡å·¥å…·å‡½æ•°
class DeviceUtils:
    """è®¾å¤‡ç®¡ç†å·¥å…·"""
    
    @staticmethod
    def get_available_devices():
        """è·å–å¯ç”¨è®¾å¤‡"""
        devices = []
        
        # CPUæ€»æ˜¯å¯ç”¨çš„
        devices.append("cpu")
        
        # æ£€æŸ¥GPU
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                devices.append(f"cuda:{i}")
                
        # æ£€æŸ¥MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            devices.append("mps")
            
        return devices
    
    @staticmethod
    def print_device_info():
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        print("ğŸ” è®¾å¤‡æ£€æµ‹ç»“æœ:")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            print("   CUDAå¯ç”¨: âœ…")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}") # type: ignore
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                name = torch.cuda.get_device_name(i)
                memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {name} ({memory:.1f}GB)")
        else:
            print("   CUDA: âŒ")
        
        # MPSä¿¡æ¯ (Apple Silicon)
        if hasattr(torch.backends, 'mps'):
            if torch.backends.mps.is_available():
                print("   MPS (Apple Silicon): âœ…")
            else:
                print("   MPS (Apple Silicon): âŒ")
        
        print(f"   æ¨èè®¾å¤‡: {DeviceUtils.get_recommended_device()}")
    
    @staticmethod
    def get_recommended_device():
        """è·å–æ¨èè®¾å¤‡"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"